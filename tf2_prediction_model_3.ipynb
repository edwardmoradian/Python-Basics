{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "print(f\"TensorFlow version {tf.__version__}\")\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "pd.set_option('display.max_rows', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Users/emoradia/OneDrive - Capgemini/Desktop/Home/kaggle/house prices/train.csv\")\n",
    "submit_data = pd.read_csv(\"C:/Users/emoradia/OneDrive - Capgemini/Desktop/Home/kaggle/house prices/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numeric columns: 37\n",
      "Number of categorical columns: 43\n",
      "The target numeric variable is SalePrice\n"
     ]
    }
   ],
   "source": [
    "num_columns = []\n",
    "cat_columns = []\n",
    "\n",
    "for column in data.keys():\n",
    "    if data[column].dtype == object:\n",
    "        cat_columns.append(column)\n",
    "    else:\n",
    "        num_columns.append(column)\n",
    "\n",
    "# remove Id variable\n",
    "num_columns.remove(\"Id\")\n",
    "        \n",
    "print(f\"Number of numeric columns: {len(num_columns)}\")\n",
    "print(f\"Number of categorical columns: {len(cat_columns)}\")\n",
    "print(\"The target numeric variable is SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn model selection to split the data\n",
    "train_data, test_data = train_test_split(data,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Samples: 292\n",
      "Train Samples: 1168\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Samples: {len(test_data)}\")\n",
    "print(f\"Train Samples: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emoradia\\Documents\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "for column in cat_columns:\n",
    "    train_data[column].fillna(value=\"Missing\",inplace=True)\n",
    "    test_data[column].fillna(value=\"Missing\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in num_columns:\n",
    "    train_data[column].fillna(train_data[column].median(), inplace = True)\n",
    "    test_data[column].fillna(test_data[column].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emoradia\\Documents\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\emoradia\\Documents\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "train_data[num_columns] = scaler.fit_transform(train_data[num_columns])\n",
    "val_data[num_columns] = scaler.transform(val_data[num_columns])\n",
    "test_data[num_columns] = scaler.transform(test_data[num_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emoradia\\Documents\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True,batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('SalePrice')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe),labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train_data, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val_data, shuffle=False, batch_size=batch_size)\n",
    "test_data['SalePrice'] = 0\n",
    "test_ds = df_to_dataset(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition']\n",
      "A batch of alleys: tf.Tensor(\n",
      "[b'Missing' b'Missing' b'Pave' b'Missing' b'Missing' b'Missing' b'Missing'\n",
      " b'Missing' b'Missing' b'Missing' b'Missing' b'Missing' b'Missing'\n",
      " b'Missing' b'Missing' b'Missing' b'Missing' b'Pave' b'Missing' b'Missing'\n",
      " b'Missing' b'Missing' b'Missing' b'Missing' b'Missing' b'Missing'\n",
      " b'Missing' b'Missing' b'Missing' b'Missing' b'Missing' b'Missing'], shape=(32,), dtype=string)\n",
      "A batch of targets: tf.Tensor(\n",
      "[-0.93873685  1.15577223 -0.34890383 -1.17948502 -0.86049369  0.11909857\n",
      " -0.52946496 -0.12621178 -0.60530063  1.84792321 -0.94475555 -0.55955848\n",
      "  0.19879825 -0.28871679 -0.44761058 -0.05759855 -0.19241752 -0.68595127\n",
      " -1.58875691  1.62523116  1.81782969 -0.52946496 -0.09009955 -0.22852975\n",
      "  0.08444287 -1.07415769 -0.26464197 -0.18038011 -0.34890383  1.32429594\n",
      "  0.67427589  0.9752111 ], shape=(32,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "    print('Every feature:', list(feature_batch.keys()))\n",
    "    print('A batch of alleys:', feature_batch['Alley'])\n",
    "    print('A batch of targets:', label_batch )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "num_columns.remove('SalePrice')\n",
    "\n",
    "for column in num_columns:\n",
    "    column = tf.feature_column.numeric_column(column)\n",
    "    feature_columns.append(column)\n",
    "\n",
    "for column in cat_columns:\n",
    "    column = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(column,list(train_data[column].unique())))\n",
    "    feature_columns.append(column)\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential([\n",
    "        feature_layer,\n",
    "        Dense(64, activation=\"relu\",name=\"Dense_2\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        Dense(128, activation=\"relu\",name=\"Dense_3\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        Dense(256, activation=\"relu\",name=\"Dense_4\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        Dense(128, activation=\"relu\",name=\"Dense_5\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        Dense(64, activation=\"relu\",name=\"Dense_6\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)      \n",
    "    ])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model summary\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',\n",
    "                optimizer=tf.keras.optimizers.Adam(.0001),\n",
    "                metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=0\n",
    "for i in (val_data.isna().sum() == 0):\n",
    "    if i == False:\n",
    "        z=z+1\n",
    "z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "916      35311\n",
       "1122    112000\n",
       "656     145500\n",
       "566     325000\n",
       "411     145000\n",
       "         ...  \n",
       "994     337500\n",
       "1069    135000\n",
       "629     168500\n",
       "177     172500\n",
       "1168    235000\n",
       "Name: SalePrice, Length: 934, dtype: int64"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 30 steps, validate for 8 steps\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.3512 - mse: 1.3621 - val_loss: 0.5798 - val_mse: 0.5858\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.2109 - mse: 1.2029 - val_loss: 0.5335 - val_mse: 0.5399\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.2483 - mse: 1.2277 - val_loss: 0.5064 - val_mse: 0.5143\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.1182 - mse: 1.1036 - val_loss: 0.4967 - val_mse: 0.5039\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.1169 - mse: 1.1069 - val_loss: 0.4860 - val_mse: 0.4927\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.1417 - mse: 1.0785 - val_loss: 0.4961 - val_mse: 0.5011\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0384 - mse: 1.0462 - val_loss: 0.5115 - val_mse: 0.5157\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.9622 - mse: 0.9604 - val_loss: 0.5218 - val_mse: 0.5226\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.9284 - mse: 0.9323 - val_loss: 0.5421 - val_mse: 0.5389\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 1s 42ms/step - loss: 1.0080 - mse: 0.9871 - val_loss: 0.5394 - val_mse: 0.5372\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.8628 - mse: 0.8410 - val_loss: 0.5538 - val_mse: 0.5557\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.9387 - mse: 0.8959 - val_loss: 0.5660 - val_mse: 0.5656\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.8703 - mse: 0.8691 - val_loss: 0.5520 - val_mse: 0.5583\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.8356 - mse: 0.8514 - val_loss: 0.5508 - val_mse: 0.5613\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.8909 - mse: 0.8959 - val_loss: 0.5327 - val_mse: 0.5446\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.7779 - mse: 0.7756 - val_loss: 0.5424 - val_mse: 0.5535\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.7484 - mse: 0.7623 - val_loss: 0.5592 - val_mse: 0.5730\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.8071 - mse: 0.8139 - val_loss: 0.5646 - val_mse: 0.5802\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.7604 - mse: 0.7682 - val_loss: 0.5304 - val_mse: 0.5471\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.7250 - mse: 0.7221 - val_loss: 0.5482 - val_mse: 0.5680\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7287 - mse: 0.6795 - val_loss: 0.5603 - val_mse: 0.5803\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.7893 - mse: 0.7912 - val_loss: 0.5638 - val_mse: 0.5791\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.7228 - mse: 0.7178 - val_loss: 0.5702 - val_mse: 0.5855\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.7096 - mse: 0.6997 - val_loss: 0.5920 - val_mse: 0.6096\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.6657 - mse: 0.6681 - val_loss: 0.5877 - val_mse: 0.6088\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.7890 - mse: 0.7151 - val_loss: 0.5599 - val_mse: 0.5810\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.6651 - mse: 0.6682 - val_loss: 0.5267 - val_mse: 0.5473\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.6171 - mse: 0.6269 - val_loss: 0.5217 - val_mse: 0.5419\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.6351 - mse: 0.6467 - val_loss: 0.5609 - val_mse: 0.5801\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.5786 - mse: 0.5767 - val_loss: 0.5434 - val_mse: 0.5588\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.6130 - mse: 0.6056 - val_loss: 0.5119 - val_mse: 0.5271\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5412 - mse: 0.5458 - val_loss: 0.4935 - val_mse: 0.5081\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.5600 - mse: 0.5683 - val_loss: 0.4956 - val_mse: 0.5111\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 0.6904 - mse: 0.6795 - val_loss: 0.5046 - val_mse: 0.5231\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.5816 - mse: 0.5887 - val_loss: 0.5012 - val_mse: 0.5220\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.6248 - mse: 0.6184 - val_loss: 0.4783 - val_mse: 0.4976\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.5866 - mse: 0.5690 - val_loss: 0.4808 - val_mse: 0.5000\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.5308 - mse: 0.5310 - val_loss: 0.4815 - val_mse: 0.5021\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.5436 - mse: 0.5476 - val_loss: 0.4656 - val_mse: 0.4850\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5029 - mse: 0.5046 - val_loss: 0.4652 - val_mse: 0.4860\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.5453 - mse: 0.5337 - val_loss: 0.4634 - val_mse: 0.4844\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.5223 - mse: 0.5118 - val_loss: 0.4622 - val_mse: 0.4814\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5639 - mse: 0.5726 - val_loss: 0.4462 - val_mse: 0.4673\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5095 - mse: 0.4973 - val_loss: 0.4355 - val_mse: 0.4568\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.5282 - mse: 0.5309 - val_loss: 0.4598 - val_mse: 0.4826\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4713 - mse: 0.4616 - val_loss: 0.4709 - val_mse: 0.4920\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.5559 - mse: 0.5563 - val_loss: 0.4816 - val_mse: 0.5034\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.5489 - mse: 0.5369 - val_loss: 0.4741 - val_mse: 0.4932\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5419 - mse: 0.5187 - val_loss: 0.4745 - val_mse: 0.4933\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4596 - mse: 0.4663 - val_loss: 0.4645 - val_mse: 0.4807\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds,validation_data=val_ds,epochs=50,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2118a90f488>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 109ms/step - loss: 0.6388 - mse: 0.6405\n",
      "mse 0.6404996\n"
     ]
    }
   ],
   "source": [
    "loss, mse = model.evaluate(test_ds)\n",
    "print(\"mse\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
